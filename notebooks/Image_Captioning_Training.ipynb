{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image Captioning with Vision Transformer - Training Notebook\n",
    "\n",
    "This notebook trains an image captioning model using Vision Transformer (ViT) encoder and Transformer decoder."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup Instructions:\n",
    "1. **Mount Google Drive** (run the cell below)\n",
    "2. **Upload your data** to Google Drive:\n",
    "   - Create folder: `/content/drive/MyDrive/image_captioning/`\n",
    "   - Upload images to: `/content/drive/MyDrive/image_captioning/data/images/`\n",
    "   - Upload captions CSV to: `/content/drive/MyDrive/image_captioning/data/captions.csv`\n",
    "3. **Run the training cell** below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mount Google Drive\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install -q transformers torch torchvision pandas pillow tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import os\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import torchvision.transforms as transforms\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "from transformers import ViTModel, ViTConfig\n",
    "import math\n",
    "\n",
    "print(\"All imports successful!\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vocabulary class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vocabulary:\n",
    "    \"\"\"Vocabulary class for caption tokenization\"\"\"\n",
    "\n",
    "    def __init__(self, freq_threshold=5):\n",
    "        self.freq_threshold = freq_threshold\n",
    "        self.itos = {0: \"<PAD>\", 1: \"<SOS>\", 2: \"<EOS>\", 3: \"<UNK>\"}\n",
    "        self.stoi = {\"<PAD>\": 0, \"<SOS>\": 1, \"<EOS>\": 2, \"<UNK>\": 3}\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.itos)\n",
    "\n",
    "    def build_vocabulary(self, sentence_list):\n",
    "        frequencies = Counter()\n",
    "        idx = 4\n",
    "        for sentence in sentence_list:\n",
    "            for word in sentence.lower().split():\n",
    "                frequencies[word] += 1\n",
    "                if frequencies[word] == self.freq_threshold:\n",
    "                    self.stoi[word] = idx\n",
    "                    self.itos[idx] = word\n",
    "                    idx += 1\n",
    "\n",
    "    def numericalize(self, text):\n",
    "        tokenized_text = text.lower().split()\n",
    "        return [\n",
    "            self.stoi[token] if token in self.stoi else self.stoi[\"<UNK>\"]\n",
    "            for token in tokenized_text\n",
    "        ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FlickrDataset(Dataset):\n",
    "    def __init__(self, root_dir, captions_file, transform=None, freq_threshold=5, split='train', split_ratio=0.8):\n",
    "        self.root_dir = root_dir\n",
    "        self.df = pd.read_csv(captions_file)\n",
    "        self.transform = transform\n",
    "        \n",
    "        unique_images = self.df['image'].unique()\n",
    "        np.random.seed(42)\n",
    "        np.random.shuffle(unique_images)\n",
    "        \n",
    "        train_size = int(len(unique_images) * split_ratio * 0.9)\n",
    "        val_size = int(len(unique_images) * split_ratio * 0.1)\n",
    "        \n",
    "        if split == 'train':\n",
    "            selected_images = unique_images[:train_size]\n",
    "        elif split == 'val':\n",
    "            selected_images = unique_images[train_size:train_size + val_size]\n",
    "        else:\n",
    "            selected_images = unique_images[train_size + val_size:]\n",
    "        \n",
    "        self.df = self.df[self.df['image'].isin(selected_images)]\n",
    "        self.imgs = self.df['image'].values\n",
    "        self.captions = self.df['caption'].values\n",
    "        \n",
    "        self.vocab = Vocabulary(freq_threshold)\n",
    "        self.vocab.build_vocabulary(self.captions.tolist())\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        caption = self.captions[index]\n",
    "        img_id = self.imgs[index]\n",
    "        img_path = os.path.join(self.root_dir, img_id)\n",
    "        img = Image.open(img_path).convert(\"RGB\")\n",
    "        \n",
    "        if self.transform is not None:\n",
    "            img = self.transform(img)\n",
    "        \n",
    "        numericalized_caption = [self.vocab.stoi[\"<SOS>\"]]\n",
    "        numericalized_caption += self.vocab.numericalize(caption)\n",
    "        numericalized_caption.append(self.vocab.stoi[\"<EOS>\"])\n",
    "        \n",
    "        return img, torch.tensor(numericalized_caption)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CaptionCollate:\n",
    "    def __init__(self, pad_idx):\n",
    "        self.pad_idx = pad_idx\n",
    "\n",
    "    def __call__(self, batch):\n",
    "        imgs = [item[0].unsqueeze(0) for item in batch]\n",
    "        imgs = torch.cat(imgs, dim=0)\n",
    "        targets = [item[1] for item in batch]\n",
    "        targets = torch.nn.utils.rnn.pad_sequence(targets, batch_first=True, padding_value=self.pad_idx)\n",
    "        return imgs, targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_transforms(image_size=224):\n",
    "    train_transform = transforms.Compose([\n",
    "        transforms.Resize((image_size, image_size)),\n",
    "        transforms.RandomHorizontalFlip(p=0.5),\n",
    "        transforms.RandomRotation(degrees=15),\n",
    "        transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "    \n",
    "    val_transform = transforms.Compose([\n",
    "        transforms.Resize((image_size, image_size)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "    \n",
    "    return train_transform, val_transform"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_len=5000, dropout=0.1):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        \n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        \n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0)\n",
    "        \n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[:, :x.size(1), :]\n",
    "        return self.dropout(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VisionTransformerEncoder(nn.Module):\n",
    "    def __init__(self, embed_dim=768, pretrained=True):\n",
    "        super(VisionTransformerEncoder, self).__init__()\n",
    "        \n",
    "        if pretrained:\n",
    "            self.vit = ViTModel.from_pretrained('google/vit-base-patch16-224-in21k')\n",
    "        else:\n",
    "            config = ViTConfig(\n",
    "                hidden_size=embed_dim,\n",
    "                num_hidden_layers=12,\n",
    "                num_attention_heads=12,\n",
    "                intermediate_size=3072,\n",
    "                image_size=224,\n",
    "                patch_size=16\n",
    "            )\n",
    "            self.vit = ViTModel(config)\n",
    "        \n",
    "        self.embed_dim = embed_dim\n",
    "\n",
    "    def forward(self, images):\n",
    "        outputs = self.vit(pixel_values=images)\n",
    "        features = outputs.last_hidden_state\n",
    "        return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerDecoder(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim=768, num_layers=6, num_heads=8,\n",
    "                 forward_expansion=4, dropout=0.1, max_length=100):\n",
    "        super(TransformerDecoder, self).__init__()\n",
    "        \n",
    "        self.embed_dim = embed_dim\n",
    "        self.word_embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "        self.positional_encoding = PositionalEncoding(embed_dim, max_length, dropout)\n",
    "        \n",
    "        decoder_layer = nn.TransformerDecoderLayer(\n",
    "            d_model=embed_dim,\n",
    "            nhead=num_heads,\n",
    "            dim_feedforward=embed_dim * forward_expansion,\n",
    "            dropout=dropout,\n",
    "            batch_first=True\n",
    "        )\n",
    "        self.transformer_decoder = nn.TransformerDecoder(decoder_layer, num_layers)\n",
    "        \n",
    "        self.fc_out = nn.Linear(embed_dim, vocab_size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, captions, encoder_out, tgt_mask=None, tgt_padding_mask=None):\n",
    "        embeddings = self.word_embedding(captions)\n",
    "        embeddings = self.positional_encoding(embeddings)\n",
    "        \n",
    "        decoder_out = self.transformer_decoder(\n",
    "            tgt=embeddings,\n",
    "            memory=encoder_out,\n",
    "            tgt_mask=tgt_mask,\n",
    "            tgt_key_padding_mask=tgt_padding_mask\n",
    "        )\n",
    "        \n",
    "        predictions = self.fc_out(decoder_out)\n",
    "        return predictions\n",
    "\n",
    "    def generate_square_subsequent_mask(self, sz, device):\n",
    "        mask = torch.triu(torch.ones(sz, sz, device=device), diagonal=1)\n",
    "        mask = mask.masked_fill(mask == 1, float('-inf'))\n",
    "        return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageCaptioningModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim=768, num_decoder_layers=6,\n",
    "                 num_heads=8, forward_expansion=4, dropout=0.1,\n",
    "                 max_length=100, pretrained_vit=True):\n",
    "        super(ImageCaptioningModel, self).__init__()\n",
    "        \n",
    "        self.encoder = VisionTransformerEncoder(embed_dim=embed_dim, pretrained=pretrained_vit)\n",
    "        self.decoder = TransformerDecoder(\n",
    "            vocab_size=vocab_size,\n",
    "            embed_dim=embed_dim,\n",
    "            num_layers=num_decoder_layers,\n",
    "            num_heads=num_heads,\n",
    "            forward_expansion=forward_expansion,\n",
    "            dropout=dropout,\n",
    "            max_length=max_length\n",
    "        )\n",
    "\n",
    "    def forward(self, images, captions, tgt_padding_mask=None):\n",
    "        encoder_out = self.encoder(images)\n",
    "        caption_len = captions.size(1)\n",
    "        tgt_mask = self.decoder.generate_square_subsequent_mask(caption_len, captions.device)\n",
    "        predictions = self.decoder(captions, encoder_out, tgt_mask, tgt_padding_mask)\n",
    "        return predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trainer class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer:\n",
    "    def __init__(self, model, train_loader, val_loader, vocab, config):\n",
    "        self.model = model\n",
    "        self.train_loader = train_loader\n",
    "        self.val_loader = val_loader\n",
    "        self.vocab = vocab\n",
    "        self.config = config\n",
    "        \n",
    "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        self.model.to(self.device)\n",
    "        \n",
    "        self.criterion = nn.CrossEntropyLoss(ignore_index=vocab.stoi[\"<PAD>\"])\n",
    "        \n",
    "        self.optimizer = optim.Adam(\n",
    "            model.parameters(),\n",
    "            lr=config['learning_rate'],\n",
    "            betas=(0.9, 0.98),\n",
    "            eps=1e-9\n",
    "        )\n",
    "        \n",
    "        self.scheduler = ReduceLROnPlateau(\n",
    "            self.optimizer,\n",
    "            mode='min',\n",
    "            factor=0.5,\n",
    "            patience=3\n",
    "        )\n",
    "        \n",
    "        self.start_epoch = 0\n",
    "        self.best_val_loss = float('inf')\n",
    "        self.train_losses = []\n",
    "        self.val_losses = []\n",
    "\n",
    "    def train_epoch(self, epoch):\n",
    "        self.model.train()\n",
    "        epoch_loss = 0.0\n",
    "        \n",
    "        pbar = tqdm(self.train_loader, desc=f'Epoch {epoch + 1}/{self.config[\"num_epochs\"]}')\n",
    "        \n",
    "        for batch_idx, (images, captions) in enumerate(pbar):\n",
    "            images = images.to(self.device)\n",
    "            captions = captions.to(self.device)\n",
    "            \n",
    "            caption_input = captions[:, :-1]\n",
    "            caption_target = captions[:, 1:]\n",
    "            \n",
    "            padding_mask = (caption_input == self.vocab.stoi[\"<PAD>\"])\n",
    "            \n",
    "            predictions = self.model(images, caption_input, padding_mask)\n",
    "            \n",
    "            predictions = predictions.reshape(-1, predictions.shape[2])\n",
    "            caption_target = caption_target.reshape(-1)\n",
    "            \n",
    "            loss = self.criterion(predictions, caption_target)\n",
    "            \n",
    "            self.optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            \n",
    "            torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=1.0)\n",
    "            \n",
    "            self.optimizer.step()\n",
    "            \n",
    "            epoch_loss += loss.item()\n",
    "            pbar.set_postfix({'loss': loss.item()})\n",
    "        \n",
    "        avg_loss = epoch_loss / len(self.train_loader)\n",
    "        return avg_loss\n",
    "\n",
    "    def validate(self):\n",
    "        self.model.eval()\n",
    "        epoch_loss = 0.0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for images, captions in tqdm(self.val_loader, desc='Validating'):\n",
    "                images = images.to(self.device)\n",
    "                captions = captions.to(self.device)\n",
    "                \n",
    "                caption_input = captions[:, :-1]\n",
    "                caption_target = captions[:, 1:]\n",
    "                \n",
    "                padding_mask = (caption_input == self.vocab.stoi[\"<PAD>\"])\n",
    "                \n",
    "                predictions = self.model(images, caption_input, padding_mask)\n",
    "                \n",
    "                predictions = predictions.reshape(-1, predictions.shape[2])\n",
    "                caption_target = caption_target.reshape(-1)\n",
    "                \n",
    "                loss = self.criterion(predictions, caption_target)\n",
    "                epoch_loss += loss.item()\n",
    "        \n",
    "        avg_loss = epoch_loss / len(self.val_loader)\n",
    "        return avg_loss\n",
    "\n",
    "    def save_checkpoint(self, epoch, is_best=False):\n",
    "        checkpoint = {\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': self.model.state_dict(),\n",
    "            'optimizer_state_dict': self.optimizer.state_dict(),\n",
    "            'scheduler_state_dict': self.scheduler.state_dict(),\n",
    "            'train_losses': self.train_losses,\n",
    "            'val_losses': self.val_losses,\n",
    "            'best_val_loss': self.best_val_loss,\n",
    "            'config': self.config\n",
    "        }\n",
    "        \n",
    "        checkpoint_path = os.path.join(self.config['checkpoint_dir'], 'latest_checkpoint.pth')\n",
    "        torch.save(checkpoint, checkpoint_path)\n",
    "        \n",
    "        if is_best:\n",
    "            best_path = os.path.join(self.config['checkpoint_dir'], 'best_model.pth')\n",
    "            torch.save(checkpoint, best_path)\n",
    "            print(f'Best model saved with validation loss: {self.best_val_loss:.4f}')\n",
    "\n",
    "    def train(self):\n",
    "        print(f'Training on device: {self.device}')\n",
    "        print(f'Number of training samples: {len(self.train_loader.dataset)}')\n",
    "        print(f'Number of validation samples: {len(self.val_loader.dataset)}')\n",
    "        print(f'Vocabulary size: {len(self.vocab)}')\n",
    "        \n",
    "        for epoch in range(self.start_epoch, self.config['num_epochs']):\n",
    "            train_loss = self.train_epoch(epoch)\n",
    "            self.train_losses.append(train_loss)\n",
    "            \n",
    "            val_loss = self.validate()\n",
    "            self.val_losses.append(val_loss)\n",
    "            \n",
    "            self.scheduler.step(val_loss)\n",
    "            \n",
    "            print(f'\\nEpoch {epoch + 1}/{self.config[\"num_epochs\"]}:')\n",
    "            print(f'Train Loss: {train_loss:.4f}')\n",
    "            print(f'Val Loss: {val_loss:.4f}')\n",
    "            print(f'Learning Rate: {self.optimizer.param_groups[0][\"lr\"]:.6f}')\n",
    "            \n",
    "            is_best = val_loss < self.best_val_loss\n",
    "            if is_best:\n",
    "                self.best_val_loss = val_loss\n",
    "            \n",
    "            self.save_checkpoint(epoch, is_best)\n",
    "            \n",
    "            history = {\n",
    "                'train_losses': self.train_losses,\n",
    "                'val_losses': self.val_losses\n",
    "            }\n",
    "            history_path = os.path.join(self.config['checkpoint_dir'], 'training_history.json')\n",
    "            with open(history_path, 'w') as f:\n",
    "                json.dump(history, f, indent=4)\n",
    "        \n",
    "        print('\\nTraining completed!')\n",
    "        print(f'Best validation loss: {self.best_val_loss:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"STARTING TRAINING\")\n",
    "print(\"=\"*80 + \"\\n\")\n",
    "\n",
    "# CONFIGURATION - Modify these paths for your setup\n",
    "config = {\n",
    "    'data_dir': '/content/drive/MyDrive/image_captioning/data/images',\n",
    "    'captions_file': '/content/drive/MyDrive/image_captioning/data/captions.csv',\n",
    "    'checkpoint_dir': '/content/drive/MyDrive/image_captioning/checkpoints',\n",
    "    'batch_size': 32,\n",
    "    'num_workers': 2,\n",
    "    'image_size': 224,\n",
    "    'embed_dim': 768,\n",
    "    'num_decoder_layers': 6,\n",
    "    'num_heads': 8,\n",
    "    'forward_expansion': 4,\n",
    "    'dropout': 0.1,\n",
    "    'max_length': 100,\n",
    "    'learning_rate': 3e-4,\n",
    "    'num_epochs': 30,\n",
    "    'pretrained_vit': True,\n",
    "    'resume': False\n",
    "}\n",
    "\n",
    "# Create checkpoint directory\n",
    "os.makedirs(config['checkpoint_dir'], exist_ok=True)\n",
    "\n",
    "# Load dataset\n",
    "print('Loading dataset...')\n",
    "train_transform, val_transform = get_transforms(config['image_size'])\n",
    "\n",
    "train_dataset = FlickrDataset(\n",
    "    root_dir=config['data_dir'],\n",
    "    captions_file=config['captions_file'],\n",
    "    transform=train_transform,\n",
    "    split='train'\n",
    ")\n",
    "\n",
    "val_dataset = FlickrDataset(\n",
    "    root_dir=config['data_dir'],\n",
    "    captions_file=config['captions_file'],\n",
    "    transform=val_transform,\n",
    "    split='val'\n",
    ")\n",
    "\n",
    "val_dataset.vocab = train_dataset.vocab\n",
    "vocab = train_dataset.vocab\n",
    "\n",
    "# Create data loaders\n",
    "pad_idx = vocab.stoi[\"<PAD>\"]\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    dataset=train_dataset,\n",
    "    batch_size=config['batch_size'],\n",
    "    num_workers=config['num_workers'],\n",
    "    shuffle=True,\n",
    "    collate_fn=CaptionCollate(pad_idx=pad_idx),\n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    dataset=val_dataset,\n",
    "    batch_size=config['batch_size'],\n",
    "    num_workers=config['num_workers'],\n",
    "    shuffle=False,\n",
    "    collate_fn=CaptionCollate(pad_idx=pad_idx),\n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "# Save vocabulary\n",
    "vocab_path = os.path.join(config['checkpoint_dir'], 'vocab.pth')\n",
    "torch.save(vocab, vocab_path)\n",
    "print(f'Vocabulary saved to {vocab_path}')\n",
    "\n",
    "# Create model\n",
    "print('Creating model...')\n",
    "model = ImageCaptioningModel(\n",
    "    vocab_size=len(vocab),\n",
    "    embed_dim=config['embed_dim'],\n",
    "    num_decoder_layers=config['num_decoder_layers'],\n",
    "    num_heads=config['num_heads'],\n",
    "    forward_expansion=config['forward_expansion'],\n",
    "    dropout=config['dropout'],\n",
    "    max_length=config['max_length'],\n",
    "    pretrained_vit=config['pretrained_vit']\n",
    ")\n",
    "\n",
    "# Count parameters\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f'Total parameters: {total_params:,}')\n",
    "print(f'Trainable parameters: {trainable_params:,}')\n",
    "\n",
    "# Create trainer and start training\n",
    "trainer = Trainer(model, train_loader, val_loader, vocab, config)\n",
    "trainer.train()\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"TRAINING COMPLETE!\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## After Training\n",
    "\n",
    "Your trained model will be saved in:\n",
    "- Best model: `/content/drive/MyDrive/image_captioning/checkpoints/best_model.pth`\n",
    "- Latest checkpoint: `/content/drive/MyDrive/image_captioning/checkpoints/latest_checkpoint.pth`\n",
    "- Vocabulary: `/content/drive/MyDrive/image_captioning/checkpoints/vocab.pth`\n",
    "- Training history: `/content/drive/MyDrive/image_captioning/checkpoints/training_history.json`"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": ".venv (3.13.5)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
